{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://alyydi:****@tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/simple\n",
      "Collecting datasets\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/ed/a5/33cf000137545a08b0a3a6ea76c8ccbd87917f78bb5d737f9f56f3b11ef6/datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[K     |████████████████████████████████| 480 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting evaluate\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/a2/e7/cbca9e2d2590eb9b5aa8f7ebabe1beb1498f9462d2ecede5c9fd9735faaf/evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 1.0 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/ed/ad/c9b96572ab7994e73c64588f8875741823f2daba70e746547fff9a2d9a54/transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.0 MB 69.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub>=0.23.0\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/60/bf/cea0b9720c32fa01b0c4ec4b16b9f4ae34ca106b202ebbae9f03ab98cd8f/huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "\u001b[K     |████████████████████████████████| 447 kB 69.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Using cached https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/b9/f8/feced7779d755758a52d1f6635d990b8d98dc0a29fa568bbe0625f18fdf3/filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting fsspec[http]<=2024.9.0,>=2023.1.0\n",
      "  Using cached https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/1d/a0/6aaea0c2fbea2f89bfd5db25fb1e3481896a423002ebe4e55288907a97a3/fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Collecting xxhash\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/62/f1/0d1bb3518dbcf4424c5d3e0b259d742e50a51753036799e8a6f1714e4fb9/xxhash-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 80.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/3d/3d/734375ec31e3e238c67dae358991db6e3177207e07d09be90036974e46f9/aiohttp-3.10.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests>=2.32.2 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.66.3 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from datasets) (4.66.6)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/fd/7f/2c3697bba5d4aa5cc2afe81826d73dfae5f049458e44732c7a0938baa673/PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (746 kB)\n",
      "\u001b[K     |████████████████████████████████| 746 kB 50.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess<0.70.17\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/ea/89/38df130f2c799090c978b366cfdf5b96d08de5b29a4a293df7f7429fa50b/multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/3f/08/bc497130789833de09e345e3ce4647e3ce86517c4f70f2144f0367ca378b/pyarrow-17.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 40.0 MB 865 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.17 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from datasets) (24.1)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.4.1\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/53/62/1d6ffba0a2bc0e6b9f5b50d421493276d9fac5ef49670d06f7b66ea73500/safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "\u001b[K     |████████████████████████████████| 436 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.21,>=0.20\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/02/52/cd7b83b6e0a1fda503ca7184b0162583de6d2f176dda9aa02abf80cb247b/tokenizers-0.20.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 59.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/6a/21/5b6702a7f963e95456c0de2d495f67bf5fd62840ac655dc451586d23d39a/attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Using cached https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/2e/a9/83692e37d8152f104333132105b67100aabfb2e96a87f6bed67f566035a7/multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/f7/d8/120cd0fe3e8530df0539e71ba9683eade12cae103dd7543e50d15f737917/aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/66/30/f9c006223feb2ac87f1826b57f2367b60aacc43092f562dab60d2312562e/frozenlist-1.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 70.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Using cached https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0; python_version < \"3.11\"\n",
      "  Using cached https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting yarl<2.0,>=1.12.0\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/c2/1e/1c78c695a4c7b957b5665e46a89ea35df48511dbed301a05c0a8beed0cc3/yarl-1.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[K     |████████████████████████████████| 319 kB 76.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: idna<4,>=2.5 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<3,>=1.21.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.8.2 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied, skipping upgrade: tzdata>=2022.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2020.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/c3/f1/69a30ff0928d07f50bdc6f0147fd9a08e80904fd3fdb711785e518de1021/propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[K     |████████████████████████████████| 213 kB 109.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: filelock, pyyaml, attrs, multidict, aiohappyeyeballs, frozenlist, aiosignal, async-timeout, propcache, yarl, aiohttp, fsspec, huggingface-hub, xxhash, dill, multiprocess, pyarrow, datasets, evaluate, safetensors, tokenizers, transformers\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 async-timeout-4.0.3 attrs-24.2.0 datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.9.0 huggingface-hub-0.26.2 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.0 pyarrow-17.0.0 pyyaml-6.0.2 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.2 xxhash-3.5.0 yarl-1.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets evaluate transformers transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alyydi/pluralsight/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-12 10:51:21.213308: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-11-12 10:51:21.213354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: VDL900341\n",
      "2024-11-12 10:51:21.213362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: VDL900341\n",
      "2024-11-12 10:51:21.213514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2024-11-12 10:51:21.213542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.183.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import keras.backend as K\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "TRACE = False\n",
    "PATIENCE = 2\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 149433.66 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 139896.12 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 149159.24 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "raw_datasets = load_dataset(\"imdb\")  # loads dataset raw\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]  # Let's see the first review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:04<00:00, 5740.53 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:04<00:00, 6235.56 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:07<00:00, 6258.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    # We are using the BERT tokenizer, specifying to PAD until the end,\n",
    "    # truncate if either 128 elements are met or the maximum from the model, which you get from the model card\n",
    "\n",
    "    return tokenizer(example[\"text\"], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(tokenized_datasets['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " inputs: (256, 128), labels: (256,)\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=['input_ids'],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "    columns=['input_ids'],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "     \n",
    "\n",
    "for inputs, labels in tf_train_dataset.take(1):\n",
    "  print(f' inputs: {inputs.shape}, labels: {labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "num_epochs = EPOCHS\n",
    "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
    "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
    "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
    "num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "lr_scheduler = PolynomialDecay(\n",
    "    initial_learning_rate=5e-5, end_learning_rate=1e-8, decay_steps=num_train_steps\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "opt = Adam(learning_rate=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # VERY important that HuggingFace models output logits\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66955010 (255.41 MB)\n",
      "Trainable params: 66955010 (255.41 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66955010 (255.41 MB)\n",
      "Trainable params: 592130 (2.26 MB)\n",
      "Non-trainable params: 66362880 (253.15 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7f4550e91dc0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x7f4550e91dc0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "98/98 [==============================] - 6389s 65s/step - loss: 0.6958 - accuracy: 0.5041 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "98/98 [==============================] - 6318s 65s/step - loss: 0.6936 - accuracy: 0.5029 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 3/3\n",
      "98/98 [==============================] - 6340s 65s/step - loss: 0.6932 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f44f1eab040>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=num_epochs, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer([\"This is the worst internet service provider\", \"Although most people say this is the worst, I like it\"], padding=True, truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2023, 2003, 1996, 5409, 4274, 2326, 10802, 102, 0, 0, 0, 0, 0], [101, 2348, 2087, 2111, 2360, 2023, 2003, 1996, 5409, 1010, 1045, 2066, 2009, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[-0.01031366,  0.00719509],\n",
       "       [-0.01029845,  0.00721388]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 69ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.49562293, 0.50437707],\n",
       "       [0.49562204, 0.50437796]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.softmax(model.predict(tokens['input_ids'])['logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 69ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.argmax(tf.math.softmax(model.predict(tokens['input_ids'])['logits']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 1906s 19s/step - loss: 0.6932 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6931858062744141, 0.5]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tf_validation_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
