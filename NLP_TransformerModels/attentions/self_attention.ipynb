{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     /home/alyydi/nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from nltk.data import find\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"word2vec_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_embedding(words):\n",
    "    word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        word2vec_sample, binary=False\n",
    "    )\n",
    "    \n",
    "    output = []\n",
    "    word_pass = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            output.append(np.array(model.word_vec(word)))\n",
    "            word_pass.append(word)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    embeddings = np.array(output)\n",
    "    del model\n",
    "    return embeddings, word_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
    "    \"\"\"Function that takes in a weight matrix and plots it with custom axis ticks\"\"\"\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    ax = sns.heatmap(weight_matrix, cmap=\"Blues\")\n",
    "    plt.xticks(np.arange(weight_matrix.shape[1]) + 0.5)\n",
    "    plt.yticks(np.arange(weight_matrix.shape[0]) + 0.5)\n",
    "    plt.title(\"Attention matrix\")\n",
    "    plt.xlabel(\"Attention score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sentence):\n",
    "    # Embed a sentence using word2vec; for example use cases only.\n",
    "    sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence)\n",
    "    words = sentence.split()\n",
    "    word_vector_sequence, words = get_word2vec_embedding(words)\n",
    "    return np.expand_dims(word_vector_sequence, axis=-1), words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./scaled_self_attention.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scaled_dot_product_attention(query, key, value):\n",
    "    d_k = key.shape[-1]\n",
    "    logits = np.matmul(query, np.swapaxes(key, -2, -1))\n",
    "    scaled_logits = logits / np.sqrt(d_k)\n",
    "    attention_weights = softmax(scaled_logits)\n",
    "    value = np.matmul(attention_weights, value)\n",
    "    return value, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_701310/108808012.py:11: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  output.append(np.array(model.word_vec(word)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6, 300, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I drink coke, but eat steak\"\n",
    "word_embeddings, words = embed_sentence(sentence)\n",
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "matmul: axis can only be used with a single shared core dimension, not with the 3 distinct ones implied by signature (n?,k),(k,m?)->(n?,m?).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m Q \u001b[38;5;241m=\u001b[39m K \u001b[38;5;241m=\u001b[39m V \u001b[38;5;241m=\u001b[39m word_embeddings\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# calculate weights and plot\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m values, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m words \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentence)\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      6\u001b[0m plot_attention_weight_matrix(attention_weights[\u001b[38;5;241m0\u001b[39m], words, words)\n",
      "Cell \u001b[0;32mIn[52], line 6\u001b[0m, in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(query, key, value)\u001b[0m\n\u001b[1;32m      4\u001b[0m scaled_logits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(d_k)\n\u001b[1;32m      5\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m softmax(scaled_logits)\n\u001b[0;32m----> 6\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value, attention_weights\n",
      "\u001b[0;31mTypeError\u001b[0m: matmul: axis can only be used with a single shared core dimension, not with the 3 distinct ones implied by signature (n?,k),(k,m?)->(n?,m?)."
     ]
    }
   ],
   "source": [
    "Q = K = V = word_embeddings\n",
    "\n",
    "# calculate weights and plot\n",
    "values, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "words = re.sub(r\"[^\\w\\s]\", \"\", sentence).split()\n",
    "plot_attention_weight_matrix(attention_weights[0], words, words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
