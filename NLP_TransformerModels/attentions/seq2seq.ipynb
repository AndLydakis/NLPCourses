{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://alyydi:****@tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/simple\n",
      "Requirement already up-to-date: nltk in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (3.9.1)\n",
      "Requirement already up-to-date: gensim==4.2.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (4.2.0)\n",
      "Requirement already up-to-date: keras-nlp in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (0.6.1)\n",
      "Requirement already up-to-date: keras-preprocessing in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (1.1.2)\n",
      "Requirement already up-to-date: tensorflow-text>=2.11 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (2.13.0)\n",
      "Requirement already satisfied, skipping upgrade: regex>=2021.8.3 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from nltk) (4.66.6)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from gensim==4.2.0) (1.24.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from gensim==4.2.0) (1.10.1)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from gensim==4.2.0) (7.0.5)\n",
      "Requirement already satisfied, skipping upgrade: keras-core in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from keras-nlp) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from keras-nlp) (24.1)\n",
      "Requirement already satisfied, skipping upgrade: absl-py in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from keras-nlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: rich in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from keras-nlp) (13.9.4)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from keras-preprocessing) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\" in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow-text>=2.11) (2.13.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-hub>=0.8.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow-text>=2.11) (0.16.1)\n",
      "Requirement already satisfied, skipping upgrade: wrapt in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim==4.2.0) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: namex in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from keras-core->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied, skipping upgrade: dm-tree in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from keras-core->keras-nlp) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from keras-core->keras-nlp) (3.11.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown-it-py>=2.2.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from rich->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions<5.0,>=4.0.0; python_version < \"3.11\" in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from rich->keras-nlp) (4.12.2)\n",
      "Requirement already satisfied, skipping upgrade: pygments<3.0.0,>=2.13.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from rich->keras-nlp) (2.18.0)\n",
      "Requirement already satisfied, skipping upgrade: gast<=0.4.0,>=0.2.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: keras<2.14,>=2.13.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (2.13.1)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (2.4.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\" in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (0.34.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.14,>=2.13.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (2.13.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2.0,>=1.24.3 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (1.67.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<2.14,>=2.13 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (2.13.0)\n",
      "Requirement already satisfied, skipping upgrade: libclang>=13.0.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (18.1.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (44.0.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading https://tratonregistry.jfrog.io/artifactory/api/pypi/ats-pypi-virtual/packages/packages/05/a6/094a2640be576d760baa34c902dcb8199d89bce9ed7dd7a6af74dcbbd62d/protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: flatbuffers>=23.1.21 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (24.3.25)\n",
      "Requirement already satisfied, skipping upgrade: astunparse>=1.6.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: tf-keras>=2.14.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorflow-hub>=0.8.0->tensorflow-text>=2.11) (2.15.1)\n",
      "Requirement already satisfied, skipping upgrade: mdurl~=0.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (0.45.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<3,>=1.6.3 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (2.36.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=1.0.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (3.0.6)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<1.1,>=0.5 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (2.32.3)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (3.7)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.8.0,>=0.7.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (0.7.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (4.9)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<6.0,>=2.0.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (5.5.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=2.1.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (2.1.5)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (3.10)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (2024.8.30)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<3,>=1.21.1 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (2.2.3)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=4.4; python_version < \"3.10\" in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (8.5.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.7.0,>=0.4.6 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=3.20 in /home/alyydi/pluralsight/venv/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text>=2.11) (3.20.2)\n",
      "\u001b[31mERROR: tensorflow 2.13.1 has requirement numpy<=1.24.3,>=1.22, but you'll have numpy 1.24.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.13.1 has requirement typing-extensions<4.6.0,>=3.6.6, but you'll have typing-extensions 4.12.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "Successfully installed protobuf-4.25.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk 'gensim==4.2.0' 'keras-nlp' 'keras-preprocessing' 'tensorflow-text>=2.11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 08:08:06.778290: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-11 08:08:06.828849: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-11 08:08:09.808942: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-11-11 08:08:09.808983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: VDL900341\n",
      "2024-11-11 08:08:09.808988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: VDL900341\n",
      "2024-11-11 08:08:09.809113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2024-11-11 08:08:09.809140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.183.6\n",
      "[nltk_data] Downloading package punkt to /home/alyydi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import preprocessing\n",
    "from textblob import TextBlob, Word\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import Model, Input\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "TRACE = False\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_data.sh\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile get_data.sh\n",
    "if [ ! -f spa.txt ]; then\n",
    "  wget -O spa.txt https://www.dropbox.com/s/ke42pnpydmy6oa6/spa.txt?dl=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVe.\n",
      "Go.\tVete.\n",
      "Go.\tVaya.\n",
      "Go.\tVáyase.\n",
      "Hi.\tHola.\n",
      "Run!\t¡Corre!\n",
      "Run.\tCorred.\n",
      "Who?\t¿Quién?\n",
      "Fire!\t¡Fuego!\n",
      "Fire!\t¡Incendio!\n"
     ]
    }
   ],
   "source": [
    "! head spa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  text = path.read_text(encoding='utf-8')\n",
    "\n",
    "  lines = text.splitlines()\n",
    "  pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "  context = np.array([context for target, context in pairs])\n",
    "  target = np.array([target for target, context in pairs])\n",
    "\n",
    "  return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pathlib\n",
    "target_raw, context_raw = load_data(pathlib.Path('./spa.txt'))\n",
    "print(context_raw[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
     ]
    }
   ],
   "source": [
    "print(target_raw[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(context_raw)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'\\xc2\\xbfSon hermanas?' b'Te dar\\xc3\\xa9 todos los que quieras.'\n",
      " b'Estos art\\xc3\\xadculos son bastante dif\\xc3\\xadciles de conseguir.'\n",
      " b'Encontr\\xc3\\xa9 la llave debajo de la alfombra.'\n",
      " b'Quedemos a las dos y media.'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'Are you sisters?' b\"I'll give you as many as you like.\"\n",
      " b'These items are rather hard to obtain.'\n",
      " b'I found the key underneath the mat.' b\"Let's meet at 2:30.\"], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "  print(example_context_strings[:5])\n",
    "  print()\n",
    "  print(example_target_strings[:5])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc2\\xbfTodav\\xc3\\xada est\\xc3\\xa1 en casa?'\n",
      "b'\\xc2\\xbfTodavi\\xcc\\x81a esta\\xcc\\x81 en casa?'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_text as tf_text\n",
    "\n",
    "example_text = tf.constant('¿Todavía está en casa?')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tf_lower_and_split_punct(text):\n",
    "  # Split accented characters.\n",
    "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "  text = tf.strings.lower(text)\n",
    "  # Keep space, a to z, and select punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "  # Add spaces around punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "  # Strip whitespace.\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Todavía está en casa?\n",
      "[START] ¿ todavia esta en casa ? [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_vocab_size = 5000\n",
    "\n",
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "context_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'the', 'i', 'to', 'you', 'tom']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)\n",
    "\n",
    "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
    "target_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[2, 13, 64, 1425, 12, 3], [2, 30, 1429, 65, 25, 5, 637, 4, 3],\n",
       " [2, 275, 1, 64, 334, 3477, 6, 866, 4, 3]]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "example_tokens = context_text_processor(example_context_strings)\n",
    "example_tokens[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Token IDs')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pcolormesh(example_tokens.to_tensor())\n",
    "plt.title('Token IDs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_text(context, target):\n",
    "  context = context_text_processor(context).to_tensor()\n",
    "  target = target_text_processor(target)\n",
    "  targ_in = target[:,:-1].to_tensor()\n",
    "  targ_out = target[:,1:].to_tensor()\n",
    "  return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   11 2432 3625   50  140    4    3    0    0]\n",
      "\n",
      "[   2    5 4244   19  184    4    0    0    0    0]\n",
      "[   5 4244   19  184    4    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
    "  print(ex_context_tok[0, :10].numpy())\n",
    "  print()\n",
    "  print(ex_tar_in[0, :10].numpy())\n",
    "  print(ex_tar_out[0, :10].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder/Decoder\n",
    "\n",
    "![alt text](./enc_dec.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, text_processor, units):\n",
    "    super().__init__()\n",
    "    self.text_processor = text_processor\n",
    "    self.vocab_size = text_processor.vocabulary_size()\n",
    "    self.units = units\n",
    "\n",
    "    # The embedding layer converts tokens to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
    "                                               mask_zero=True)\n",
    "\n",
    "    # The RNN layer processes those vectors sequentially.\n",
    "    self.rnn = tf.keras.layers.Bidirectional(\n",
    "        merge_mode='sum',\n",
    "        layer=tf.keras.layers.GRU(units,\n",
    "                            # Return the sequence and state\n",
    "                            return_sequences=True,\n",
    "                            recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "  def call(self, x):\n",
    "\n",
    "    # 2. The embedding layer looks up the embedding vector for each token.\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # 3. The GRU processes the sequence of embeddings.\n",
    "    x = self.rnn(x)\n",
    "\n",
    "    # 4. Returns the new sequence of embeddings.\n",
    "    return x\n",
    "\n",
    "  def convert_input(self, texts):\n",
    "    texts = tf.convert_to_tensor(texts)\n",
    "    if len(texts.shape) == 0:\n",
    "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
    "    context = self.text_processor(texts).to_tensor()\n",
    "    context = self(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens, shape (batch, s): (64, 16)\n",
      "Encoder output, shape (batch, s, units): (64, 16, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder = Encoder(context_text_processor, UNITS)\n",
    "ex_context = encoder(ex_context_tok)\n",
    "\n",
    "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "![alt text](./cross_prod_at.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, x, context):\n",
    "\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    # Cache the attention scores for plotting later.\n",
    "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
    "    self.last_attention_weights = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])  # Residual Connection\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context sequence, shape (batch, s, units): (64, 16, 256)\n",
      "Target sequence, shape (batch, t, units): (64, 14, 256)\n",
      "Attention result, shape (batch, t, units): (64, 14, 256)\n",
      "Attention weights, shape (batch, t, s):    (64, 14, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "embed = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(),\n",
    "                                  output_dim=UNITS, mask_zero=True)\n",
    "ex_tar_embed = embed(ex_tar_in)\n",
    "\n",
    "result = attention_layer(ex_tar_embed, ex_context)\n",
    "\n",
    "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
    "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
    "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
    "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer.last_attention_weights[0].numpy().sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The decoder\n",
    "The decoder's job is to generate predictions for the next token at each location in the target sequence.\n",
    "\n",
    "* It looks up embeddings for each token in the target sequence.\n",
    "* It uses an RNN to process the target sequence, and keep track of what it has generated so far.\n",
    "* It uses RNN output as the \"query\" to the attention layer, when attending to the encoder's output.\n",
    "* At each location in the output it predicts the next token.\n",
    "When training, the model predicts the next word at each location. So it's important that the information only flows in one direction through the model. The decoder uses a unidirectional (not bidirectional) RNN to process the target sequence.\n",
    "\n",
    "When running inference with this model it produces one word at a time, and those are fed back into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./uni_rnn.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, text_processor, units):\n",
    "    super().__init__()\n",
    "    self.text_processor = text_processor\n",
    "    self.vocab_size = text_processor.vocabulary_size()\n",
    "    self.word_to_id = tf.keras.layers.StringLookup(\n",
    "        vocabulary=text_processor.get_vocabulary(),\n",
    "        mask_token='', oov_token='[UNK]')\n",
    "    self.id_to_word = tf.keras.layers.StringLookup(\n",
    "        vocabulary=text_processor.get_vocabulary(),\n",
    "        mask_token='', oov_token='[UNK]',\n",
    "        invert=True)\n",
    "    self.start_token = self.word_to_id('[START]')\n",
    "    self.end_token = self.word_to_id('[END]')\n",
    "\n",
    "    self.units = units\n",
    "\n",
    "\n",
    "    # 1. The embedding layer converts token IDs to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                                units, mask_zero=True)\n",
    "\n",
    "    # 2. The RNN keeps track of what's been generated so far.\n",
    "    self.rnn = tf.keras.layers.GRU(units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # 3. The RNN output will be the query for the attention layer.\n",
    "    self.attention = CrossAttention(units)\n",
    "\n",
    "    # 4. This fully connected layer produces the logits for each\n",
    "    # output token.\n",
    "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n",
    "\n",
    "  def call(self, context, x, state=None, return_state=False):\n",
    "\n",
    "    # 1. Lookup the embeddings\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # 2. Process the target sequence.\n",
    "    x, state = self.rnn(x, initial_state=state)\n",
    "\n",
    "    # 3. Use the RNN output as the query for the attention over the context.\n",
    "    x = self.attention(x, context)\n",
    "    self.last_attention_weights = self.attention.last_attention_weights\n",
    "\n",
    "    # Step 4. Generate logit predictions for the next token.\n",
    "    logits = self.output_layer(x)\n",
    "\n",
    "    if return_state:\n",
    "      return logits, state\n",
    "    else:\n",
    "      return logits\n",
    "\n",
    "# Stuff we will need for inference\n",
    "\n",
    "  def get_initial_state(self, context):\n",
    "    batch_size = tf.shape(context)[0]\n",
    "    start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "    embedded = self.embedding(start_tokens)\n",
    "    return start_tokens, done, self.rnn.get_initial_state(embedded)[0]\n",
    "\n",
    "  def tokens_to_text(self, tokens):\n",
    "    words = self.id_to_word(tokens)\n",
    "    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "    result = tf.strings.regex_replace(result, '^ *START*', '')\n",
    "    result = tf.strings.regex_replace(result, ' *END*$', '')\n",
    "    return result\n",
    "\n",
    "  def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
    "    logits, state = self(context, next_token, state = state, return_state=True)\n",
    "\n",
    "    if temperature == 0.0:\n",
    "      next_token = tf.argmax(logits, axis=-1)\n",
    "    else:\n",
    "      logits = logits[:, -1, :]/temperature\n",
    "      next_token = tf.random.categorical(logits, num_samples=1)\n",
    "\n",
    "    # If a sequence produces an `end_token`, set it `done`\n",
    "    done = done | (next_token == self.end_token)\n",
    "    # Once a sequence is done it only produces 0-padding.\n",
    "    next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
    "\n",
    "    return next_token, done, state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
